# Fantastic Food and Where to Find Them



### 0. Group Members

- Linghui Wu (linghuiwu)
- Mingtao Gao (mingtao)
- Shengwenxin Ni (nswxin)
- Yanwei Pan (panyw)



### 1. Overview

The project aims to meet the needs of consumers in Chicago who have special food restrictions, allergies, and religious beliefs to consider. Compared to traditional food search engines, we enable customers to look for food products not only based on the dietary labels ("dairy-free," "vegan," "kosher," etc.) but also according to ingredients contained and nutritional information, such as trans-fat per serving. Firstly, we collect nearly 17200 pieces of product information from three major grocery stores in Chicago - Whole Foods, Trader Joe's and Jewel Osco -  through web-scraping, during which regular expression has been employed to grab keywords we need.  After the cleaning process, data from different sources has been merged and utilized to establish SQL database including tables named "product" and "store". Then, we built the backend for the grocery search tool by connecting the SQLite database to the frontend, a user-friendly Django web interface. Through the interface, users can enter and select dietary requirements, and the product information, along with other useful information, including store locations, will be displayed.



### 2. Structure

The project consists of two major parts: the Back End and the Front End.


#### 2.1 Back End

The `BackEnd` folder is composed of three modules, the web-crawler, the data cleaner, and the database constructor.

- Web Crawler

In each of the `JOCrawler.py`, `TradeJoesCrawler.py` and `WholeFoodsCrawler.py`, we created a class and defined helper functions to collect and store product and store information from the three grocery stores mentioned above. 

To facilitate the replication of web scraping, we provide `test.py` to run in the command line. You may specify the number of observations and the name of the store and it would generate sample csv files. Please see intructional write-up in the `BackEnd\Crawler` folder for more details.

- Data Cleaner

Raw data gathered by crawler are stored in the relative  `storename_prod.csv` and `storename_store.csv` files. 

Using pandas, `DataCleaning.py` reads the csv files and processes the data by droping missing values and unifying the column names. Then, it merges the cleaned data from three sources into two dataframes representing all the products and store information, and saves them as `product.csv` and `store.csv`. Those files will be later used to construct tables in the database.

- Database Constructor

`database.sql` includes SQL codes to construct the `product` and `store` tables, to import data from csv files into the according tables, and to set the NULL values in the table.

By entering the command `.read database.sql` in the shell for SQLite in terminal, the user can construct the database for the project `foodsearch.sqlite3`.


#### 2.2 Front End

The front end is mainly composed of two parts, the grocery search tool `search_items.py` and the Django web interface, which we referenced the codes from the second assignment.

`search_items.py` takes in `args_to_ui`, a dictionary generated by the information entered by users on the interface, and then it produces query statements to retrive results from the `foodsearch.sqlite3` database. To display information neatly, we only selected the product name, the store name, the store address, and additional information related to the dietary restrition that are selected or entered by the user. For example, if the user chooses less than 10g sugars contained in one serving, the interface will display product and store information as well as the amount of sugars contained per serving.

To run the Django web interface for the grocery search tool, you can enter `python3 manage.py runserver` in the command line inside the `FrontEnd` folder. Once the interface is started, you can access the search engine by pointing a browser to `http://127.0.0.1:8000/`.



### 3. Accomplishment

We initially planned to develop a grocery search engine catering to individuals with dietary restrictions. After scraping and cleaning data from Whole Foods, Trader Joe's, and Instacart websites, we successfully built a database containing product and store information and a user-interactive interface that implement keywords queries based on food ingredients, nutrition contents, dietary labels, and store information.

We have achieved the primary goal of this project. However, there are still some aspects we can make further improvements:

- Provide more detailed dietary labels

Due to the variation on how product information is documented in each website and the broad definition of those dietary labels, we only considered  "organic", "vegan", "dairy-free", and "kosher". However, we may identify more features by performing content analysis on the ingredients, such as distinguishing "vegan" foods from "vegetarian" ones.

- Offer more precise store locations

For now, the program can only provide consumers with nearby grocery stores by their zip code and finding stores that share the same first four-digit zipcode. However, we hope to convert the store address into longitude and latitude and utilize Google Map API to calculate a more precise distance between the target store and the user. 

- Design a more attractive and user-friendly web interface

Due to our limited knowledge about web development, we implemented the UI primarily based on the codes in the previous assignment. With more in-depth learning on HTML and front-end development, we hope to display thorough details of each product neatly. 

